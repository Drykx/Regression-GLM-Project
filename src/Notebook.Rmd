---
title: "CommunFileFINAL"
author: "JS"
date: "2023-12-23"
output: html_document
---

# Data Exploration

## Cleaning 

### Packages + Files:
```{r}

source("helpers.R")

library(jsonlite)
library(ggplot2)
library(styler)
library(lubridate)
library(glmnet)
library(SMPracticals)
library(dplyr)
library(patchwork)
library(cowplot)
library(MASS)


Butts <- import_data0()
Butts_reduced <- import_final_data()


```

## Covariates

```{r}
summary(Butts)
summary(Butts_reduced)

```

### Number of butts
```{r}


Butts_reduced$y1 <- 2* sqrt(1/4 * Butts_reduced$y + 1)
Butts_reduced$y2 <- log( Butts_reduced$y + 1)

#pdf("distributtiontranfbutt.pdf", width = 15 , height = 5)  # Specify width and height if needed


density_butts1 <- ggplot(Butts_reduced, aes(x = y)) +
  geom_density(fill = "skyblue", color = "black", alpha = 0.7) +
  labs(x = "y", y = "Density") +
  theme_minimal() +
    theme(axis.text.x = element_text(size = 10),  
    axis.text.y = element_text(size = 10))  
density_butts2 <- ggplot(Butts_reduced, aes(x = y1)) +
  geom_density(fill = "skyblue", color = "black", alpha = 0.7) +
  labs(x = "y1", y = "Density") +
  theme_minimal() +
    theme(axis.text.x = element_text(size = 10),  
    axis.text.y = element_text(size = 10))  

density_butts3 <- ggplot(Butts_reduced, aes(x = y2)) +
  geom_density(fill = "skyblue", color = "black", alpha = 0.7) +
  labs(x = "y2", y = "Density") +
  theme_minimal() +
    theme(axis.text.x = element_text(size = 10),  
    axis.text.y = element_text(size = 10))  

combined_density_plots <- density_butts1 / density_butts2 / density_butts3 +
  plot_layout(guides = 'collect') +
  plot_annotation(title = "Combined Histograms of butts and its transformations")

combined_density_plots


#dev.off()  # Close the PNG device

```


### Length: 

```{r}

x = unique(Butts_reduced$Length)

mean_value <- mean(x)
median_value <- median(x)

cat("Mean:", mean_value, "\n")
cat("Median:", median_value, "\n")
quantiles <- quantile(x, c(0.25, 0.5, 0.75))

cat("25th Percentile:", quantiles[1], "\n")
cat("50th Percentile (Median):", quantiles[2], "\n")
cat("75th Percentile:", quantiles[3], "\n")

grouped_data <- aggregate(y ~ Beach, data = Butts_reduced, mean)
grouped_data$Length <- Butts_reduced$Length[match(grouped_data$Beach, Butts_reduced$Beach)]
  
grouped_data$transf_length <- log(grouped_data$Length)

lm2 <- lm(y ~ transf_length - 1, data = grouped_data)

cat("coefficients of the linear fit:", lm2$coefficients)

#pdf("LengthOffset.pdf", width = 12 , height = 5)  # Specify width and height if needed

# Scatterplot with fitted line
plot(grouped_data$transf_length, grouped_data$y, 
     main = "Scatterplot of the Mean Number of Butts against Beach Length",
     xlab = "Logarithmic Beach Length", ylab = "Mean Number of Butts relative to Beach",
     pch = 16, col = "#4E79A7", cex = 1.3)  # Adjusting point shape, color, and size
abline(lm2, col = "#E15759", lwd = 1.7)  # Adjusting line color and width

grid()


#dev.off()  # Close the PNG device

```


### Months/Year 

Here we will check our data and see patterns and their dynamics: Months/Years

Interesting, we exactly have 12 months. Therefore we could drop the variable year.

With the assumptions that there is no change through the years.
(we could have a slight increase (linear growth per year for a constant to determine)). 
Given the lack of data we cannot determine the value of the constant.

```{r}

unique_combinations <- unique(Butts_reduced[c("Year", "Month")])
print(unique_combinations)

months_counts <- table(Butts_reduced$Month)
print(months_counts)

```

The numbers of data points seem to be normally distributed around the month of June. Weird result, since it should be randomly collected.

Results:

-Every month seem to follow a distribution that is similar, expect that the number of outliers are more frequent in the month 5,6,7,8

Explanations:

-More data therefore more outliers, or relation between the month of high influence and outliers.

```{r}

Butts_reduced$Month <- factor(Butts_reduced$Month, levels = c("1", "2","3","4","5","6","7","8","9","10","11","12"))

hist_data <- split(Butts_reduced$y, Butts_reduced$Month)
bin_size <- 5
num_breaks <- 50

par(mfrow = c(2, 2), mar = c(4, 4, 2, 1))  
for (i in 1:length(hist_data)) {
  hist(hist_data[[i]], main = names(hist_data)[i], xlab = "y", col = "lightblue", border = "black",
       breaks = seq(0, max(Butts_reduced$y) + bin_size,  length.out = num_breaks + 1), probability = TRUE)
}

```


Which is why we defined this season, to have more precision for our prediction intervals:
```{r}

#pdf("bench_query_sort.pdf", width= 12 , height=5)
# Splitting the data
hist_data <- split(Butts_reduced$y1, Butts_reduced$Season.custom)

# Define color palette
color_palette <- c("blue", "green", "coral", "goldenrod")

# Define the names for each season
Mot <- c("January/February/March", "April/May/June", "July/August/September", "October/November/December")

# Create an empty plot with appropriate limits and labels
plot(range(hist_data[[1]]), type = "n", main = "Density Plots by Season", xlab = "y1", ylab = "Density",
     ylim = c(0, 0.6),xlim = c(0,25), cex.axis = 1.25, cex.lab = 1.25)
grid()

# Add density lines for each season
for (i in 1:length(hist_data)) {
  lines(density(hist_data[[i]]), col = color_palette[i], lwd = 2.5)
}

# Add a legend to the plot
legend("topright", legend = Mot, fill = color_palette, cex = 0.8)

#dev.off()
```
```{r}
library(ggplot2)
library(tidyr)
library(ggplot2)
library(tidyr)

# Assuming hist_data is already defined as in your previous code
# Combine all your data into one data frame and create a new column for Season
all_data <- do.call(rbind, lapply(names(hist_data), function(x) data.frame(Season = x, y1 = hist_data[[x]])))
all_data$Season <- factor(all_data$Season, levels = names(hist_data))

# Define line types
line_types <- c("solid", "dashed", "dotdash", "longdash")

# Create the ggplot
ggplot(all_data, aes(x = y1, linetype = Season)) +
  geom_density() +
  scale_linetype_manual(values = line_types) +
  scale_color_manual(values = "black") +
  labs(title = "Density Plots by Season", x = "y1", y = "Density", linetype = "Season") +
  theme_minimal() +
  theme(legend.position = "topright") +
  guides(color = FALSE) # Turn off the color guide

# Save the plot as a PDF
ggsave("bench_query_sort.pdf", width = 12, height = 5)

```

## Outliers 

PLOTTING STANDARDIZED RESIDUALS AGAINST COVARIATES

Length:

-Rhein_Beach near Tinguely Museum_Bolger. O_Sigrist F. length 186. Is a crazy outlier.

-There is still 4 other beaches who are outliers.

Months:

-Looks more dirty around month 6, but it could be because of the outliers.


```{r}

lm_model <- lm(y ~ Beach + Length + Month + Season.custom  -1, data = Butts)

standardized_residuals <- rstandard(lm_model)

plot(standardized_residuals, main = "Standardized Residuals", ylab = "Standardized Residuals")

```
Dirtiest beach in Switzerland. Reason, close to the border, influenced
by outside factors specially in Summer. Could include a geospatial covariate.

```{r}

z <- Butts[Butts$Beach == "Rhein_Beach near Tinguely Museum_Bolger. O_Sigrist F.",]

plot(z$Month, z$y, main = "Standardized Residuals of the dirtiest beach in Switzerland", 
     xlab = "Months", ylab = "Standardized Residuals")

```
Same analysis but without Basel outliers:

Other outliers:

Z�\_richsee-Feldeggstr-B�\_nningerSand Rhein_Basel
zuerichsee_zurich_kullg greifensee_greifensee_simmenc

```{r}

lm_model <- lm(y ~  Beach + Season.custom + Month +  offset(log(Length)) -1, data = Butts_reduced)


standardized_residuals <- rstandard(lm_model)
plot(standardized_residuals, main = "Standardized Residuals", ylab = "Standardized Residuals")

# Plot standardized residuals against one covariate (replace 'x1' with the desired covariate)
plot(Butts_reduced$Length, standardized_residuals, main = "Standardized Residuals vs Length", 
     xlab = "Length", ylab = "Standardized Residuals")

# Add a horizontal line at y = 0 for reference
abline(h = 0, col = "red", lty = 2)

# Plot standardized residuals against one covariate (replace 'x1' with the desired covariate)
plot(Butts_reduced$Month, standardized_residuals, main = "Standardized Residuals vs Months", 
     xlab = "Months", ylab = "Month Residuals")

# Add a horizontal line at y = 0 for reference
abline(h = 0, col = "red", lty = 2)

boxplot(standardized_residuals ~ Butts_reduced$Season.custom, main = "Standardized Residuals vs Season", 
        xlab = "Season", ylab = "Standardized Residuals")


```

# Model fitting

##GLM 

### Davision Model

 *log(mu) = Month + Beach + offset(log(Length)) -1*

- Poor fit for big values of butts. Problem of the Poisson models is that the variance is equal to the mean. Where as here it is not the case 

------------------------------------------------------------------------

COOKS DISTANCE: p=4 In the theory it is said to consider a point as
influential if its Cook's distance is greater than (8 /(n - 2\*p)). Here
since n is so small, a quarter of the points are bigger. Therefore I
took the value 1 and we get 3 results.

Tells me 141, 235, 422 are influential.

```{r}
anova(fit <- glm(y ~ Month + Beach + offset(log(Length))  - 1, family = poisson(link = "log"), data = Butts_reduced))


plot(fit)
fit.diag <- glm.diag(fit)
plot.glm.diag(fit, fit.diag)

aic <- AIC(fit)
bic <- BIC(fit)
cooksd <- cooks.distance(fit)

cat("\n")
print(paste("AIC:", aic))
print(paste("BIC:", bic))

# Identify influential observations

p = 4
C = 8 / (length(cooksd)-2*p)
print(paste("Threshold for Cook's Distance:", C))
print(paste("Used threshold for Cook's Distance:", 1))

influential_points <- which(cooksd > 1 ) # You can adjust the threshold as needed

# Print indices of influential points
print("Indices of influential points:")
print(cooksd[influential_points])
print("Values of Butts in the influential points:")
print(Butts_reduced$y[influential_points])
```

Solution:

    - Parametric Modelling 
    - Quasi likelihood estimation
    
### Davision Model v2 

-We still have the same problem from the transformation, the fitted value does not exceed 5

 *log(mu) = Month + Beach + Season x day + Settlement + offset(log(Length)) -1*
 
 y1:
```{r}

#pdf("GLM.pdf", width= 15 , height=5)

par(mfrow = c(1, 3))  
par(cex.axis = 1.3)
par(cex.lab = 1.3)

hist(Butts_reduced$y1, col = "lightblue", main = "", xlab = "y1", border = "white", breaks =20)

fit <- glm(y1 ~ Month + Beach + (Season.custom* Day) + (Season.custom * Settlement) + offset(log(Length))  - 1, family = quasipoisson(link = "log"), data = Butts_reduced)

hist(predict(fit), col = "salmon", main = "", xlab = "y1 predicted with Quasi-Poisson", border = "white",breaks=20)
fit <- glm(y1 ~ Month + Beach + (Season.custom* Day) + (Season.custom * Settlement) + offset(log(Length))  - 1, family = negative.binomial(theta = 5,link = "log"), data = Butts_reduced)

hist(predict(fit), col = "lightgreen", main = "", xlab = "y1 predicted with NegativeBinomial", border = "white",breaks=20)


# Reset the par settings
par(mfrow = c(1, 1))
#dev.off()
```

y2:
```{r}

#pdf("GLM.pdf", width= 15 , height=5)

par(mfrow = c(1, 3))  
par(cex.axis = 1.4)
par(cex.lab = 1.5)

hist(Butts_reduced$y2, col = "lightblue", main = "", xlab = "y1", border = "white", breaks =20)

fit <- glm(y2 ~ Month + Beach + (Season.custom* Day) + (Season.custom * Settlement) + offset(log(Length))  - 1, family = quasipoisson(link = "log"), data = Butts_reduced)

hist(predict(fit), col = "salmon", main = "", xlab = "H_{QuasiPoisson} * y1", border = "white")

fit <- glm(y2 ~ Month + Beach + (Season.custom* Day) + (Season.custom * Settlement) + offset(log(Length))  - 1, family = negative.binomial(theta = 10,link = "log"), data = Butts_reduced)

hist(predict(fit), col = "lightgreen", main = "", xlab = "H_{NegativeBinomial(theta=4.7) * y1", border = "white")

# Reset the par settings
par(mfrow = c(1, 1))

#dev.off()
```

QuasiPoisson
```{r}

anova(fit <- glm(y1 ~ Month + Beach + (Season.custom* Day) + (Season.custom * Settlement) + offset(log(Length))  - 1, family = quasipoisson(link = "log"), data = Butts_reduced),test = "Cp")

aic <- AIC(fit)
bic <- BIC(fit)
cat("\n")
print(paste("AIC:", aic))
print(paste("BIC:", bic))

cooksd <- cooks.distance(fit)
p = 4
C = 8 / (length(cooksd)-2*p)
print(paste("Threshold for Cook's Distance:", C))
print(paste("Used threshold for Cook's Distance:", 1))

influential_points <- which(cooksd > 1 ) # You can adjust the threshold as needed

# Print indices of influential points
print("Indices of influential points:")
print(cooksd[influential_points])
print("Values of Butts in the influential points:")
print(Butts_reduced$y[influential_points])

cat("Model Deviance:", fit$deviance, "\n")


```

```{r}
theta_values <- seq(1, 30, by = 1) 

best_theta <- NA
best_bic <- Inf

for (theta in theta_values) {
  fit <- glm(y1 ~ Month + Beach + (Season* Day) + (Season * Settlement) + offset(log(Length)) - 1, 
             family = negative.binomial(theta = theta, link = "log"), data = Butts_reduced)
  current_bic <- BIC(fit)
  # Update best theta if current model has lower AIC
  if (current_bic < best_bic) {
    best_theta <- theta
    best_bic <- current_bic
  }
}

cat("Optimal theta:", best_theta, "\n")
cat("BIC for optimal theta:", best_bic, "\n")

```
```{r}
theta_values <- seq(1, 30, by = 1) 

best_theta <- NA
best_dev <- Inf

for (theta in theta_values) {
  fit <- glm(y1 ~ Month + Beach + (Season* Day) + (Season * Settlement) + offset(log(Length)) - 1, 
             family = negative.binomial(theta = theta, link = "log"), data = Butts_reduced)
  current_dev <- fit$deviance
  # Update best theta if current model has lower AIC
  if (current_dev < best_dev) {
    best_theta <- theta
    best_dev <- current_dev
  }
}

cat("Optimal theta:", best_theta, "\n")
cat("Deviance for optimal theta:", best_dev, "\n")

```

Negative binomial:
```{r}

anova(fit <- glm(y1 ~ Month + Beach + (Season* Day) + (Season * Settlement) + offset(log(Length))  - 1, family = negative.binomial(theta =5,link = "log"), data = Butts_reduced),test = "Cp")


aic <- AIC(fit)
bic <- BIC(fit)

cat("\n")
print(paste("AIC:", aic))
print(paste("BIC:", bic))
cooksd <- cooks.distance(fit)
p = 4
C = 8 / (length(cooksd)-2*p)
print(paste("Threshold for Cook's Distance:", C))
print(paste("Used threshold for Cook's Distance:", 2*C))

influential_points <- which(cooksd > 2*C ) # You can adjust the threshold as needed

# Print indices of influential points
print("Indices of influential points:")
print(cooksd[influential_points])
print("Values of Butts in the influential points:")
print(Butts_reduced$y[influential_points])

cat("Model Deviance:", fit$deviance, "\n")

```

## Mixed Model

Implementation of random effects

    - Fixed and random effects
    - Nested and crossed effects 

## Spline smoothing 

    - Smoothing splines
    - Cubic splines
    - Natural cubic splines
    
## Additive Models 

## GAM

```{r}

fit <- mgcv::gam(y1~s(Beach, bs="re") + Beach + Month + Season.custom*Day +Season.custom * Settlement + offset(log(Length)) - 1, data = Butts_reduced, family=quasipoisson, link="log", method = "REML")

print(paste0("Model fit: Deviance = ", deviance(fit), 
             ", R-squared = ", summary(fit)$r.sq,
             ", df_res = ", fit$df.residual))

print(fit$model.null)
```


```{r}

fit0 <- mgcv::gam(y1~s(Beach, bs="re") + Beach + Month + offset(log(Length)) - 1, data = Butts_reduced, family=quasipoisson, link="log", method = "REML") 

fit <- mgcv::gam(y1~s(Beach, bs="re") + Beach + Month + Season.custom*Day +Season.custom * Settlement + offset(log(Length)) - 1, data = Butts_reduced, family=quasipoisson, link="log", method = "REML")

```

```{r}

fit1 <- mgcv::gam(y1~ Beach + Month + s( Beach, bs = "re", by= Settlement) + offset(log(Length)) - 1, data = Butts_reduced, family=quasipoisson, link="log",method = "REML")

fit2 <- mgcv::gam(y1~ Beach + Month + s( Beach, bs = "re", by= Season.custom) + offset(log(Length)) - 1, data = Butts_reduced, family=quasipoisson, link="log",method = "REML")

fit3 <- mgcv::gam(y1~ Beach + Month + s( Beach, bs = "re", by= Month) + offset(log(Length)) - 1, data = Butts_reduced, family=quasipoisson, link="log",method = "REML")

```

```{r}

print(paste0("Model fit: Deviance = ", deviance(fit0), 
             ", R-squared = ", summary(fit0)$r.sq,
             ", df_res = ", fit0$df.residual))

print(paste0("Model fit: Deviance = ", deviance(fit), 
             ", R-squared = ", summary(fit)$r.sq,
             ", df_res = ", fit$df.residual))

print(paste0("Model fit1: Deviance = ", deviance(fit1), 
             ", R-squared = ", summary(fit1)$r.sq,
             ", df_res = ", fit1$df.residual))

print(paste0("Model fit2: Deviance = ", deviance(fit2), 
             ", R-squared = ", summary(fit2)$r.sq,
            ", df_res = ", fit2$df.residual))

print(paste0("Model fit3: Deviance = ", deviance(fit3), 
             ", R-squared = ", summary(fit3)$r.sq,
            ", df_res = ", fit3$df.residual))
```

```{r}

anova(fit,fit0, test = "Chisq")

```


```{r}
summary(fit3)
```
```{r}
summary(fit)
```

```{r}

anova(fit4)
mgcv::gam.check(fit4)
```






